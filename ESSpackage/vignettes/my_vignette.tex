\documentclass{article}

\usepackage{amsmath, amsthm, amssymb}
\usepackage[round]{natbib}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage[left=2.5cm, right=2.5cm, top=3cm, bottom=3.00cm]{geometry}

% The line below tells R to use knitr on this.
%\VignetteEngine{knitr::knitr}

\title{Elliptical Slice Sampling}
\author{Francesca Panero \and Xuewen Yu}

\usepackage{Sweave}
\begin{document}
\input{my_vignette-concordance}

\maketitle

\begin{abstract}
We present the R package `ESS', which provides an implementation of the Markov chain Monte Carlo method presented by Murray, Adams and MacKay in 2010 in the paper ``Elliptical slice sampling" published in Journal of Machine Learning Research $\cite{MAM}$.
\end{abstract}

\section{Introduction}

Slice sampling was introduced by Neal in 2003 \cite{Neal2003} in the homonym paper published in the Annals of Statistics. Neal's idea was to present a new approach to sample from a probability density function, that tried to overcome some drawbacks of the two major MCMC methods of sampling: the Gibbs sampler and the Metropolis Hastings method.

The aim of these three methods is to sample from a probability density function from which we are not able to sample directly. Their basic idea is to construct a Markov chain that have as stationary distribution the target one we care about, and samples from this chain will eventually come from the desired distribution. 
%Gibbs sampler exploits the possibility of sampling from the full conditional density functions of each parameter given all the others (that are usually simpler to deal with than the original joint distribution), since the complete vector of parameters will eventually be sampled from the joint distribution of these. No tuning of parameters of the chain is required, but we must be able to sample from these conditional distributions in order to implement the method, and sometimes this could not be the case. Nevertheless, some algorithms to overcome this problem have been proposed, in particular ARS (adaptive rejection sampling) and MARS (adaptive rejection Metropolis sampling).
%Metropolis Hasting, instead, initializes the chain with a value sampled from a proposal distribution from which we are able to sample from and that attains some nice links with the target distribution. This sample can be accepted or rejected according to a probability that depends on the likelihood of the new candidate point and the previous one. This method presents some scale parameters for which there does not exist a certain rule of decision; moreover, the proposal distribution does not always come from a straightforward reasoning.

%Slice sampling tries to overcome these flaws in the previous methods. 
The basic idea of slice sampling lies in the fact that if we want to sample $x$ from a distribution $p(x)$ that is proportional to a certain function $f(x)$, it would be sufficient just to sample uniformly from the area below $f(x)$.
By defining an auxiliary random variable $y$, we exploit Gibbs sampler to sample from the two full conditional distributions. In particular, $y|x$ is distributed as an $Uniform(0,f(x))$ and $x|y$ is distributed as an uniform on the so called slice $S=\lbrace x: y<f(x)\rbrace$. The joint density of $x$ and $y$ is, then $$p(x,y)=\begin{cases}1/Z\quad&\text{ for }0<y<f(x)\\0\quad&\text{ otherwise}\end{cases}$$where $Z=\int f(x)dx$. From this it can easily be seen that $p(x)=\int_0^{f(x)} p(x,y)dy=1/Zf(x)$ as desired. 
%Sampling on the slice can be difficult, and it is sometimes substituted with some update for $x$ which leaves invariant the uniform distribution.

\section{Elliptical Slice Sampling}

Elliptical Slice Sampling is a particular case of MCMC methods that avoids the tuning of parameters, simpler and often faster than other methods to sample from the posterior distribution of models with multivariate Gaussian prior.

Let $\mathbf{f}$ be the vector of latent variables distributed as a Gaussian distribution with zero vector mean and covariance matrix $\Sigma$: $$\mathbf{f}\sim\mathcal{N}(\mathbf{f};\mathbf{0}, \Sigma)=|2\pi\Sigma|^{-1/2}\text{exp}\left(-\frac{1}{2}\mathbf{f}^T\Sigma^{-1}\mathbf{f}\right)$$ Let $$L(\mathbf{f})=p(\text{data}|\mathbf{f})$$ be the likelihood function. Our target distribution is the posterior of this model: $$p^*(\mathbf{f})\propto \mathcal{N}(\mathbf{f};\mathbf{0}, \Sigma)L(\mathbf{f}).$$

Neal, in 1999, introduced the idea of a Metropolis-Hastings algorithm for this type of problems, by proposing as new state 
$$\mathbf{f^{'}}=\sqrt{1-\epsilon^2}\mathbf{f}+\epsilon\pmb{\nu},\qquad\pmb{\nu}\sim\mathcal{N}(\mathbf{0}, \Sigma)$$ where $\epsilon\in[-1,1]$ is a step-size parameter, and its varying defines half of an ellipse, having $\pmb{\nu}$ fixed. When $\epsilon=0$ we are resampling the same value. The probability of accepting the move is $$p(\text{accept})=\text{min}(1,L(\mathbf{f^{'}})/L(\mathbf{f}))$$ otherwise the next state is still $\mathbf{f}$. A drawback of this algorithm is that $\epsilon$ needs to be tuned so that the Markov chain can mix efficiently. A representation of the whole ellipse gives a richer choice of updates, so we will exploit: $$\mathbf{f^{'}}=\pmb{\nu}\text{sin}\theta+\mathbf{f}\text{cos}\theta$$ that represents the ellipse centered in the origin of the axes and passing through $\mathbf{f}$ and $\pmb{\nu}$, and $\theta$ is a new step-size parameter.

To avoid the tuning of the parameter, the idea is to augment the prior to make $\theta$ a random variable and sample it exploiting the slice sampling. The augmented model becomes
\begin{align*}
\pmb{\nu}_0&\sim\mathcal{N}(\mathbf{0},\Sigma)\\
\pmb{\nu}_1&\sim\mathcal{N}(\mathbf{0},\Sigma)\\
\theta&\sim \text{Uniform}[0,2\pi]\\
\mathbf{f^{'}}&=\pmb{\nu}_0\text{sin}\theta+\pmb{\nu}_1\text{cos}\theta
\end{align*}
which assures that the distribution of $\bf{f}$ is always $\mathcal{N}(\mathbf{0},\Sigma)$. The new target distribution is $$p^*(\pmb{\nu}_0,\pmb{\nu}_1,\mathbf{f})\propto\mathcal{N}(\pmb{\nu}_0;\mathbf{0},\Sigma)\mathcal{N}(\pmb{\nu}_1;\mathbf{0},\Sigma)L(\mathbf{f}(\pmb{\nu}_0,\pmb{\nu}_1,\theta))$$
The slice sampling exploits the following algorithm:\\

\begin{algorithm}
\caption{ESS Algorithm}\label{ess}
\hspace*{\algorithmicindent} \textbf{Input}: $\mathbf{f},\,\text{log}L$ \\
\hspace*{\algorithmicindent} \textbf{Output}: $\mathbf{f^'}\sim p^*(\mathbf{f})$ 
\begin{algorithmic}[1]
%\Procedure{Euclid}{$a,b$}
  \State Sample from $p(\pmb{\nu}_0,\pmb{\nu}_1,\theta|\pmb{\nu}_0\text{sin}\theta+\pmb{\nu}_1\text{cos}\theta=\mathbf{f})$:
\begin{align*}
\theta &\sim\text{Uniform}[0,2\pi]\\
\pmb{\nu}&\sim\mathcal{N}(\mathbf{0},\Sigma)\\ 
\pmb{\nu}_0 &\leftarrow\mathbf{f}\text{sin}\theta+\pmb{\nu}\text{cos}\theta\\
\pmb{\nu}_1 &\leftarrow\mathbf{f}\text{cos}\theta-\pmb{\nu}\text{sin}\theta
\end{align*}
  \State Sample $\theta$ using slice sampling on $p^*(\theta|\pmb{\nu}_0,\pmb{\nu}_1)\propto L(\pmb{\nu}_0\text{sin}\theta+\pmb{\nu}_1\text{cos}\theta)$ 
  \State $\mathbf{f^{'}}=\pmb{\nu}_0\text{sin}\theta+\pmb{\nu}_1\text{cos}\theta$
%\EndProcedure
\end{algorithmic}
\end{algorithm}

To avoid proposing $\pmb{\nu}_{0}$ and $\pmb{\nu}_{1}$ in every iteration of the algorithm and to shrink the range of $\theta$ after every iteration, Murray, Adams and MacKay \cite{MAM} 
suggested a mature algorithm as follows:

\begin{algorithm}
\caption{Neater ESS Algorithm}\label{ess*}
\hspace*{\algorithmicindent} \textbf{Input}: $\mathbf{f},\,\text{log}L$ \\
\hspace*{\algorithmicindent} \textbf{Output}: $\mathbf{f^'}\sim p^*(\mathbf{f})$ 
\begin{algorithmic}[1]
\State Sample $\pmb{\nu}\sim\mathcal{N}(\pmb{\nu};\mathbf{0},\Sigma)$ (this defines the ellipse centered in the origine passing through $\mathbf{f}$ and $\pmb{\nu}$).
\State Define the slice by sampling the height $y$:
\begin{align*}
u &\sim\text{Uniform}[0,1]\\
\text{log}y &\leftarrow \text{log}L(\mathbf{f})+\text{log}u
\end{align*}
\State Define the bracket for the angles:
\begin{align*}
\theta &\sim\text{Uniform}[0,2\pi]\\
[\theta_{\text{min}},\theta_{\text{max}}]&\leftarrow [\theta-2\pi,\theta]
\end{align*}
\State Propose a new status $\mathbf{f^{'}}\leftarrow\mathbf{f}\text{cos}\theta+\pmb{\nu}\text{sin}\theta$
\If{$\text{log}L(\mathbf{f^{'}})>\text{log}y$}
    \State Accept: \textbf{return} $\mathbf{f^{'}}$
\Else{ Shrink the bracket:}
    \If{$\theta<0$} $\,\theta_{min}\leftarrow\theta$ 
    \Else $\,\theta_{max}\leftarrow\theta$
  \State $\theta\sim\text{Uniform}[\theta_{min}, \theta_{max}]$
  \State Go to 4.
\end{algorithmic}
\end{algorithm}


\subsection{Reversibility of Elliptical Slice Sampling}

The Elliptical Slice Sampling algorithm updates $\bf{f}$ by proposing $\bf{f_1}$,...,$\bf{f_k}$,... based on $\theta_1$,...,$\theta_k$,..., until $\bf{f_K}$ satisfies step 5 in the algorithm. Then $\bf{f_K} = \bf{f'}$. This procedure is reversible. We can verify this by showing

\begin{equation}
p(\mathbf{f}, y, \pmb{\nu},\lbrace\theta_k\rbrace)=p(\mathbf{f'}, y, \pmb{\nu'},\lbrace\theta_k^{'}\rbrace)
\end{equation}

This joint distribution is the multiplication of target distribution and proposals of ${\theta_k}$ and $\pmb{\nu}$:

\begin{align}\label{newtarget}
p(\mathbf{f}, y, \pmb{\nu},\lbrace\theta_k\rbrace)&=p^*(\mathbf{f})p(y|\mathbf{f})p(\pmb{\nu})p(\lbrace\theta_k\rbrace|\mathbf{f},\pmb{\nu},y)\propto\\
&\propto\mathcal{N}(\mathbf{f};\mathbf{0},\Sigma)L(\mathbf{f})\mathcal{N}(\pmb{\nu};\mathbf{0},\Sigma)\frac{1}{L(\mathbf{f})}p(\lbrace\theta_k\rbrace|\mathbf{f},\pmb{\nu},y)
\end{align}

Another variable $\pmb{\nu}_k$ is added which is updated in every iteration, such that:

\begin{equation}
\pmb{\nu}_k = \pmb{\nu} \text{cos}(\theta_k) - \mathbf{f}\text{sin}(\theta_k)\qquad k=1,...,K
\end{equation}

Then we have:

\begin{equation}\label{gaussians}
\mathcal{N}(\pmb{\nu}_k;\mathbf{0},\Sigma)\mathcal{N}(\mathbf{f}_k;\mathbf{0},\Sigma)=\mathcal{N}(\pmb{\nu};\mathbf{0},\Sigma)\mathcal{N}(\mathbf{f};\mathbf{0},\Sigma) \qquad\forall{k}
\end{equation}

This can be easily proved by computing Jacobian matrix, which has determinat equalling 1 in this case.

$\theta = 0$ corresponds to the location of $\bf{f}$. When we reverse the procedure, the location of $\bf{f'}$ corresponds to $\theta' = 0$. So the relative angle of the location of $\bf{f}$ is $-\theta_K$. By generating the same $y$, $\nu$ and angle:

\begin{equation}
\theta_k' = \theta_k-\theta_K
\end{equation}

we can return to $\bf{f}$. So the following equation holds:

\begin{equation}\label{angles}
p(\lbrace\theta_k\rbrace|\mathbf{f},\pmb{\nu},y)=p(\lbrace\theta_k^{'}\rbrace|\mathbf{f'},\pmb{\nu'},y)
\end{equation}

This will lead to the reversibility of the algorithm by substituting %$\ref{angles}$ 
and %$\ref{gaussians}$ 
in %$\ref{newtarget}$.

\section{Experiments}
In this section, we will validate the elliptical slice sampling algorithm on 2 models: Gaussian regression and Log Gaussian Cox process, and compare this algorithm with the Metropolis-Hastings algorithm adapted by Neal \cite{Neal}.
  
\subsection{Model Description}
\subsubsection{Gaussian Regression}

Observations $y_n$ are drawn from Normal distribution with mean $f_n$ and variance $\sigma_n^2$, for $n = 1,...,N$. Let $N$ denote the sample size, $D$ denote the number of dimensions. $\bf{f}$ $= (f_1,...,f_N) \sim{N(0,\Sigma)}$. To simulate $\bf{f}$, we define the covariance matrix as 
\begin{equation}
\Sigma_{i,j} = \sigma_{f}^{2}exp(-\frac{1}{2}\sum_{d=1}^{D}(x_{d,i} - x_{d,j})^2/l^2)
\end{equation}
Covariance matrix $\Sigma$ is computed by inputing $\bf{X}$, which is a $D\times{N}$ matrix. $\bf{X} = [x_1,...,x_N]^T$, where the column vector $\bf{x_n}$, $n = 1,...,N$, is the 'feature' vector with $D$ dimensions and $f_n$ is a function of $x_n$. We will draw $\bf{x_n}$ from a D-dimensional unit hypercube for all $n$. 
We can simulate observations $y_n$ after generating $f_n$ and fixing $\sigma_{n}^2$ and $\sigma_{f}^2$. $y_1,...,y_N$ are $\it{i.i.d}$ normal variables and $\bf{y|f} \sim{N(\bf{f},\sigma_n^2I)}$, so the likelihood function as a function of $\bf{f}$ is:
\begin{equation}
L_r(\bf{f}) = \prod_{n=1}^{N} \it{N(y_n;f_n,\sigma_n^2)}
\end{equation}


\subsubsection{Log Gaussian Cox process}
  Cox process is a "doubly stochastic" Poisson process with a stochastic intensity measure \cite{Moller}. 
  Log Gaussian Cox process is introduced by Moller \cite{Moller} 
  as the Cox process where the logarithm of the internsity function is a Gaussian process. Mathematically, let $y_n$ denote the observations. Then $y_n \sim{Poisson(\lambda_n)}$ with mean $\lambda_n$. The intensity function can be estimated given the log Gaussian Cox process observation within a bounded subset. This means we can partition the space finitely into $N$ bins and $y_n$ is the number of events in bin $n$ for all $n = 1,...,N$. We assume that every bin has a constant intensity function $\lambda_n$. Let $m$ be the offset to the log mean $\lambda_n$, and define it as the sum of the mean log-intensity of the Poisson process and the log of the bin size \cite{MAM}. 
  \begin{equation}
  y_n|f_n \sim{Poisson(exp(f_n + m))}
  \end{equation}
  \begin{equation}
  \bf{f} \sim{N(0,\Sigma)}
  \end{equation}
  
  $\Sigma$ is defined in Equation (7). Then the likelihood of $\bf{y}$ is:
  \begin{equation}
  L_p(\bf{f}) = \prod_{n=1}^{N}\frac{\lambda_{n}^{y_{n}} exp(-\lambda_n)}{y_n!}, \lambda_n = e^{f_n+m}
  \end{equation}
  
\subsection{Implementation and Results}
\subsubsection{Gaussian Regression}

Let $l=1$, $\sigma_{f}^2 = 1$, $\sigma_{n}^2 = 0.3^2$. Firstly, we validate the Elliptical Slice Sampling algorithm on Gaussian regression model when $\bf{f}$ is bivariate Normal variable. In this case, let $N = 2$ and $D = 1$ such that $\{\bf{x}_n\}_{n=1}^{2}$ is one dimensional. Since both prior distribution and likelihood function are Gaussian, the posterior distribution of $\bf{f}$ should be Gaussian. We perform Henze-Zirkler's test to assess whether the outputs follow Bivariate Normal distribution. This is based on the measure of distance, which is nonnegative, between the characteristic function of the multivariate normality and the empirical characteristic function. The distribution of the test statistic is approximately log normal. We achieved a p-value much larger than 0.05, which indicates that there is no strong evidence to reject the null hypothesis that the outputs of the algorithm follows multivariate normal distribtuion. We can visualise the distribution of outputs in Figure 1. As a necessary condition for mutivariate normality, each variable should have normal distribution. This can be verified by QQ-plot as shown in Figure 2.

%\begin{figure} [h!]
%\centering
%\includegraphics[width = 10cm]{3dplot.pdf}
%\caption{Perspective (left) and Contour (right) plots for bivariate outputs}
%\end{figure}
%\begin{figure} [h!]
%\centering
%\includegraphics[width = 10cm]{qqplot.pdf}
%\caption{QQ plot of $f_1$ and $f_2$ from Elliptical Slice Sampling algorithm }
%\end{figure}

Then we assess the performance of the algorithm on Gaussian regression when $N=200$, i.e. $\bf{f}$ is 200-dimensional. As stated in the model description section, $\bf{f}$ can be generated by inputing $X$ to covariance matrix. So we first simulated datasets $X$. In order to compare the performance of algorithm for different dimensions, i.e. $D$, of feature vectors, 3 synthetic datasets $X_1$,$X_2$,$X_3$ will be simulated with $D = 1,5,10$ respectively. 
%\begin{figure} [h!]
%\centering
%\includegraphics[width = 10cm]{figure1.pdf}
%\caption{Trace plot of log likelihood of 333 points in the first 1000 iterations by taking every 3 iterations}
%\end{figure} 
%\begin{figure}[h!]
%\centering
%\includegraphics[width = 10cm]{tracef_1.pdf}
%\caption{Trace plot of the first dimension dimension of output $f$ of 100000 iterations}
%\end{figure}
The traces of log-likelihoods and the first dimension of outputs for $D=1$ are shown in Figure 3 and 4 respectively, which reveal frequent jumps in both chains. The effective sample size for it is 3177.

\subsubsection{Log Gaussian Cox process}

Data of mining disasters are provided by Jarret \cite{Jarrett}. There were 191 events happening during 40550 days which were partitioned into 811 bins each of which contains 50 days. Given the date of every event, the number of event happened in each bin can be computed, i.e. $y_n$. Let $l=13516$, $\sigma_f^2 = 1$, $N = 811$, $D=1$ and $m=log(191/811)$. The trace of the first dimension of the output is shown below.
%\begin{figure}[h!]
%\centering
%\includegraphics[width = 10cm]{figureminedata.pdf}
%\caption{Trace plot of the first dimension dimension of output $f$ of 20000 iterations}
%\end{figure}

\subsection{Comparison}
We will compare the performance Elliptical Slice Sampling algorithm with the Metropolis-Hasting algorithm introduced by Neal \cite{Neal} 
for both Gaussian regression model and log Gaussian Cox process model.The effective sample size, likelihood and CPU time for running 100000 iterations after 10000 burn in will be evaluated. We will perform 100 runs to discover the mean and standard deviation of the evaluations. 


\begin{thebibliography}{12}

\bibitem{Jarrett}
	Jarrett, R. G.  (1979)
	\emph{A note on the intervals between coal-mining disasters},
	Biometrika.

\bibitem{Moller}
	M{\o}ller, J., Syversveen, A. R., Waagepetersen  (1998)
	\emph{Log Gaussian Cox processes},
	Scandinavian Journal of Statistics.

\bibitem{MAM}
	Murray, I., Adams, R. P., and MacKay, D. J. C. (2010)
	\emph{Elliptical slice sampling},
	Journal of Machine Learning Research.
	
\bibitem{Neal}
	Neal, R. M. (1999)
	\emph{Regression and Classification Using
Gaussian Process Priors},
	Bayesian Statistics 6 (475-501), OU Press.
	
\bibitem{Neal2003}
	Neal, R. M. (2003)
	\emph{Slice sampling},
	Annals of Statistics.

\end{thebibliography}

\end{document}








